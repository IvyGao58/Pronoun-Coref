# Word embeddings.
glove_300d_filtered {
  path=glove_emb/glove.840B.300d.txt.filtered
  size=300
}

glove_300d_2w {
  path=glove_emb/glove_50_300_2.txt
  size=300
}

zhihu_300d_filtered {
  path=chinese_emb/sgns.zhihu.word.filtered
  size=300
}

renmin_300d {
  path=chinese_emb/sgns.renmin.bigram-char
  size=300
}

# Main configuration.
best {
GPU=1
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="glove_emb/char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  knowledge_pruning=false
  attention=true
  number=true
  type=true
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

p_m_coref {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path="chinese_emb/char_vocab.chinese.txt"
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.001
  knowledge_as_feature=false
  order_as_features=false
  order_length=10
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  apply_biaffine=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  use_PMS_attention=false
  use_MS_gcn=true

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  biaffine_out_features=2
  biaffine_score_weight = 0.7

  # Other.
  train_path=data/law/train.jsonlines
  eval_path=data/law/dev.jsonlines
  test_path=data/law/test.jsonlines
  lm_path=elmoformanylangs/elmo_chinese_cache_v2.hdf5
  title_map_path=data/law/title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}


p_s_coref {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path="chinese_emb/char_vocab.chinese.txt"
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.000001
  knowledge_as_feature=false
  order_as_features=false
  order_length=10
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  apply_biaffine=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  use_PMS_attention=false
  use_MS_gcn=true

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  biaffine_out_features=2
  biaffine_score_weight = 0.7

  # Other.
  train_path=data/law/train.p2s.jsonlines
  eval_path=data/law/dev.p2s.jsonlines
  test_path=data/law/test.p2s.jsonlines
  lm_path=elmoformanylangs/elmo_chinese_cache_p2s.hdf5
  title_map_path=data/law/title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

add_gcn_coref {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path="chinese_emb/char_vocab.chinese.txt"
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.00001
  knowledge_as_feature=false
  order_as_features=true
  order_length=10
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  apply_biaffine=true
  use_PMS_attention=false
  use_PMS_gcn=true
  use_xor_matrix=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  size_per_head_with_gcn=820
  biaffine_out_features=2
  biaffine_score_weight = 0.4

  # Other.
  train_path=data/law/train.p2sSort.jsonlines
  eval_path=data/law/dev.p2sSort.jsonlines
  test_path=data/law/test.p2sSort.jsonlines
  lm_path=elmoformanylangs/elmo_chinese_cache_p2sSort.hdf5
  title_map_path=data/law/title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

best1 {
GPU=2
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  knowledge_pruning=false
  attention=true
  number=true
  type=true
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

knowledge_as_feature {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=true
  use_elmo=true
  random_sample_training=true
  apply_knowledge=false
  softmax_pruning=false
  knowledge_pruning=false
  attention=true
  number=true
  type=true
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

no-attention {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=false
  knowledge_pruning=false
  attention=false
  number=true
  type=true
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

no-number {
GPU=3
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=false
  knowledge_pruning=false
  attention=true
  number=false
  type=true
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

no-type {
GPU=2
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=false
  knowledge_pruning=false
  attention=true
  number=true
  type=false
  nsubj= true
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

no-sp {
GPU=2
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  char_embedding_size=8
  char_vocab_path="char_vocab.english.txt"
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  model_heads=true
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0000001
  knowledge_as_feature=false
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=false
  knowledge_pruning=false
  attention=true
  number=true
  type=true
  nsubj= false
  dobj=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100

  # Other.
  train_path=data/sample/train.jsonlines
  eval_path=data/sample/dev.jsonlines
  lm_path=elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  report_frequency=100
  log_root=logs
}

