# Word embeddings.
glove_300d_filtered {
  path = glove_emb/glove.840B.300d.txt.filtered
  size = 300
}

glove_300d_2w {
  path = glove_emb/glove_50_300_2.txt
  size = 300
}

zhihu_300d_filtered {
  path = chinese_emb/sgns.zhihu.word.filtered
  size = 300
}

renmin_300d {
  path = chinese_emb/sgns.renmin.bigram-char
  size = 300
}

# Main configuration.
best {
GPU = 1
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "glove_emb/char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = true
  knowledge_pruning = false
  attention = true
  number = true
  type = true
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

best_zh {
GPU = 0
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  emb_filter_widths = [1, 2, 3]
  emb_filter_size = 128
  char_embedding_size = 8
  char_vocab_path = "chinese_emb/char_vocab.chinese.txt"
  context_embeddings = ${zhihu_300d_filtered}
  head_embeddings = ${renmin_300d}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_multi_span = false
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  # softmax_threshold = 0.000001
  softmax_threshold = 0.1
  softmax_biaffine_threshold = 0.01
  knowledge_as_feature = false
  order_as_features = true
  order_length = 10
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = true
  softmax_biaffine_pruning = true
  knowledge_pruning = true
  attention = true
  number = true
  plurality = false
  type = true
  nsubj= false
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.2
  lexical_dropout_rate = 0.2
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  size_per_head = 256
  biaffine_out_features = 2

  # Other.
  train_path = data/law/train.jsonlines
  eval_path = data/law/dev.jsonlines
  test_path = data/law/test.jsonlines
  lm_path = elmoformanylangs/elmo_chinese_cache_v2.hdf5
  title_map_path = data/law/title_map.txt
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 100
  eval_matrix_path = eval_matrix.txt
  report_frequency = 100
  k_fold = 5
  log_root = logs
  max_patience = 20
}

best1 {
GPU = 2
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = true
  knowledge_pruning = false
  attention = true
  number = true
  type = true
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

knowledge_as_feature {
GPU = 0
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = true
  use_elmo = true
  random_sample_training = true
  apply_knowledge = false
  softmax_pruning = false
  knowledge_pruning = false
  attention = true
  number = true
  type = true
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

no-attention {
GPU = 0
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = false
  knowledge_pruning = false
  attention = false
  number = true
  type = true
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

no-number {
GPU = 3
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = false
  knowledge_pruning = false
  attention = true
  number = false
  type = true
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

no-type {
GPU = 2
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = false
  knowledge_pruning = false
  attention = true
  number = true
  type = false
  nsubj= true
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

no-sp {
GPU = 2
  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 50
  top_span_ratio = 1

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  model_heads = true
  coref_depth = 2
  lm_layers = 3
  lm_size = 1024
  coarse_to_fine = true

  # knowledge_hyperparameters
  softmax_threshold = 0.0000001
  knowledge_as_feature = false
  use_elmo = true
  random_sample_training = true
  apply_knowledge = true
  softmax_pruning = false
  knowledge_pruning = false
  attention = true
  number = true
  type = true
  nsubj= false
  dobj = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  softmax_threashold = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = data/sample/train.jsonlines
  eval_path = data/sample/dev.jsonlines
  lm_path = elmo_cache.hdf5
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
}

